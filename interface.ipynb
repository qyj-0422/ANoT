{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------#\n",
    "###构建模型###\n",
    "#--------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "graph_name = 'GDELT'\n",
    "root_path = 'data/'\n",
    "dataset = graph_name+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "\n",
    "import pickle\n",
    "from graph import Graph\n",
    "graph = Graph(graph_name, idify=False)\n",
    "pickle.dump(graph, open(root_path + dataset + \"graph_new.pickle\", \"wb\"))\n",
    "# 4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Searcher object to search for a model (set of rules) and build the raw model\n",
    "from searcher import Searcher\n",
    "searcher = Searcher(graph)\n",
    "model = searcher.build_model()\n",
    "pickle.dump(model, open(root_path + dataset + \"static_model_new.pickle\", \"wb\"))\n",
    "model.print_stats()\n",
    "#1m40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the temporal model\n",
    "temporal_model, candidate_p, candidate_t = searcher.build_temporal_model(model)\n",
    "pickle.dump(temporal_model, open(root_path + dataset + \"temporal_model_new.pickle\", \"wb\"))\n",
    "temporal_model.print_stats(temporal_model)\n",
    "#10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### directly read preserved files\n",
    "\n",
    "import pickle\n",
    "graph_name = 'YAGO'\n",
    "root_path = 'data/'\n",
    "dataset = graph_name+'/'\n",
    "\n",
    "graph = pickle.load(open(root_path + dataset + \"graph_new.pickle\", \"rb\"))\n",
    "model = pickle.load(open(root_path + dataset + \"static_model_new.pickle\", \"rb\"))\n",
    "temporal_model = pickle.load(open(root_path + dataset + \"temporal_model_new.pickle\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initlize the detection function\n",
    "import pickle\n",
    "from anomaly_detector import AnomalyDetector\n",
    "from model_updater import ModelUpdater\n",
    "\n",
    "detector = AnomalyDetector(temporal_model)\n",
    "\n",
    "def read_file(input_file):\n",
    "    raw_data = []\n",
    "    for fact in input_file.readlines():\n",
    "        s, r, o, t = fact.strip().split('\t')[:4]\n",
    "        s = int(s)\n",
    "        r = int(r)\n",
    "        o = int(o)\n",
    "        t = int(t)\n",
    "        raw_data.append((s, r, o, t))\n",
    "    return raw_data\n",
    "\n",
    "valid_pos, test_pos = read_file(open(root_path + dataset + 'valid.txt', 'r')), read_file(open(root_path + dataset + 'test.txt', 'r'))\n",
    "valid_t_2_C_neg, test_t_2_C_neg = pickle.load(open(root_path + dataset + '/conceptual_errors.pickle', 'rb'))\n",
    "valid_t_2_T_neg, test_t_2_T_neg = pickle.load(open(root_path + dataset + '/time_errors.pickle', 'rb'))\n",
    "valid_t_2_M_neg, test_t_2_M_neg = pickle.load(open(root_path + dataset + '/missing_errors.pickle', 'rb'))\n",
    "\n",
    "valid_t_2_pos = {}\n",
    "test_t_2_pos = {}\n",
    "for sample in valid_pos:\n",
    "    s, r, o, t = sample\n",
    "    if t not in valid_t_2_pos.keys():\n",
    "        valid_t_2_pos[t] = []\n",
    "    valid_t_2_pos[t].append((int(s), int(r), int(o), int(t)))\n",
    "\n",
    "\n",
    "for sample in test_pos:\n",
    "    s, r, o, t = sample\n",
    "    if t not in test_t_2_pos.keys():\n",
    "        test_t_2_pos[t] = []\n",
    "    test_t_2_pos[t].append((int(s), int(r), int(o), int(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect concept+time+missing anomaly\n",
    "# next_step_rules_list = list(next_step_rules)\n",
    "# for rule_pair in next_step_rules_list[:max_rule]:\n",
    "# #for rule_pair in rules.keys():\n",
    "        #    self.updater.update_temporal_model(self.temporal_model, rule_pair[0], rule_pair[1], rules[rule_pair], sample_size)\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.dummy import freeze_support, Manager\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, accuracy_score, f1_score\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def get_f1(p, r):\n",
    "        if (p+r) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2*(p*r)/(p+r)\n",
    "\n",
    "def get_f05(p, r):\n",
    "        if (p+r) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.25*(p*r)/(0.25*p+r)\n",
    "        \n",
    "def get_metric(pred, y, anomaly_type):\n",
    "    precision, recall, threshold = precision_recall_curve(y, pred)\n",
    "    # find the best threshold by F1 score\n",
    "\n",
    "    f1s = [get_f1(precision[i], recall[i]) for i in range(len(precision))]\n",
    "    f05s = [get_f05(precision[i], recall[i]) for i in range(len(precision))]\n",
    "    optimal_index = np.argmax(f05s)\n",
    "    '''\n",
    "    while precision[optimal_index] == 1.0:\n",
    "        f05s[optimal_index] = 0\n",
    "        optimal_index = np.argmax(f05s)\n",
    "    '''\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "\n",
    "    optimal_P_by_fscore, optimal_R_by_fscore, optimal_F1_by_fscore, optimal_F05_by_fscore, optimal_T_by_fscore = precision[optimal_index], recall[optimal_index], f1s[optimal_index], f05s[optimal_index], threshold[optimal_index]\n",
    "    optimal_ACC_by_fscore = accuracy_score(y, pred > optimal_T_by_fscore)\n",
    "\n",
    "    print('----- Result stats -----' + anomaly_type)\n",
    "    print('Number of test samples: ' + str(len(y)))\n",
    "    print('Number of anomaly samples: ' + str(sum(y)))\n",
    "    print('-----------------------')\n",
    "    print('Best result by F1-score:')\n",
    "    print('P: ' + str(optimal_P_by_fscore))\n",
    "    print('F05: ' + str(optimal_F05_by_fscore))\n",
    "    print('AUC: ' + str(auc))\n",
    "\n",
    "     # find the best threshold by 0.95 Precision\n",
    "    \n",
    "    filter_precision = np.array(precision)\n",
    "    filter_precision[filter_precision < 0.80] = 10\n",
    "    f1s = [get_f1(precision[i], recall[i]) for i in range(len(precision))]\n",
    "    f05s = [get_f05(precision[i], recall[i]) for i in range(len(precision))]\n",
    "    optimal_index = np.argmin(filter_precision)\n",
    "    if precision[optimal_index] == 1.0:\n",
    "        filter_precision = np.array(precision)\n",
    "        filter_precision[filter_precision == 1.0] = 0\n",
    "        optimal_index = np.argmax(filter_precision)\n",
    "    \n",
    "    for ind in range(len(precision)):\n",
    "        if recall[ind] >= 0.30 and precision[ind] >= 0.80:\n",
    "            optimal_index = ind\n",
    "            break\n",
    "\n",
    "    optimal_P_by_acc, optimal_R_by_acc, optimal_F1_by_acc, optimal_T_by_acc = precision[optimal_index], recall[optimal_index], f1s[optimal_index], threshold[min(optimal_index, len(threshold) - 1)]\n",
    "    optimal_ACC_by_acc = accuracy_score(y, pred > optimal_T_by_acc)\n",
    "\n",
    "# init parameters\n",
    "hop = 2 # ICEWS14 2; ICEWS05 2; YAGO 10 GD 2\n",
    "span = 2000 # ICEWS14 1000; ICEWS05 2000; YAGO 50 GD 12000\n",
    "time_specific = False # ICEWS14 F; ICEWS05 F; YAGO F\n",
    "span_t = 2000 # ICEWS14 200; ICEWS05 2000; YAGO 50 GD 12000\n",
    "span_m = 2000 # ICEWS14 150; ICEWS05 2000; YAGO 50 GD 12000\n",
    "max_rule = 10000 # ICEWS14 10000; ICEWS05 10000; YAGO 10000 GD 10000\n",
    "step = 2 # ICEWS14 2; ICEWS05 2; YAGO 1 GD 2\n",
    "aux_score = True # ICEWS14 T; ICEWS05 T; YAGO F GD F\n",
    "aux_score_m = False # ICEWS14 F; ICEWS05 F; YAGO F GD F\n",
    "distribution_aware = False # ICEWS14 T; ICEWS05 F; YAGO F GD F\n",
    "and_or = 'and' # ICEWS14 or; ICEWS05 and; YAGO or GD and\n",
    "update_sample_size = 10 # ICEWS14 10; ICEWS05 10; YAGO 20 GD 5 \n",
    "update_span = 3000 # ICEWS14 200; ICEWS05 3000; YAGO 20 GD 12000\n",
    "pred_list_C = [] # for concept anomaly\n",
    "label_list_C = []\n",
    "\n",
    "pred_list_T = [] # for time anomaly\n",
    "label_list_T = []\n",
    "\n",
    "pred_list_M = [] # for missing anomaly\n",
    "label_list_M = []\n",
    "\n",
    "time_specific_pred_list_C = []\n",
    "time_specific_label_list_C = []\n",
    "time_specific_pred_list_T = []\n",
    "time_specific_label_list_T = []\n",
    "time_specific_pred_list_M = []\n",
    "time_specific_label_list_M = []\n",
    "\n",
    "all_proj_rules = []\n",
    "def update_helper_valid(m):\n",
    "    raw_score, project_rules = detector.score_edge_temporal((int(m[0]), int(m[1]), int(m[2]), int(m[3])), hop = hop, max_span = span_t, and_or = and_or, max_step = step, aux_score = aux_score, dsa = distribution_aware, max_rule = max_rule, file_type='valid')\n",
    "    detector.update(m, project_rules, update_sample_size, update_span)\n",
    "\n",
    "def update_helper_test(x_y):\n",
    "    m = x_y[0]\n",
    "    project_rules = x_y[1]\n",
    "    detector.update(m, project_rules, update_sample_size, update_span)\n",
    "\n",
    "def concept_detect_helper(fact_label_t):\n",
    "    m = fact_label_t[0]\n",
    "    label = fact_label_t[1]\n",
    "    t = fact_label_t[2]\n",
    "    raw_score = detector.score_edge((int(m[0]), int(m[1]), int(m[2]), int(t)), hop = hop, max_span = span, and_or = and_or, time_specific = time_specific)\n",
    "    pred = sigmoid(raw_score)\n",
    "\n",
    "    return [pred, label]\n",
    "\n",
    "def temporal_detect_helper(fact_label_t):\n",
    "    m = fact_label_t[0]\n",
    "    label = fact_label_t[1]\n",
    "    t = fact_label_t[2]\n",
    "    raw_score, project_rules = detector.score_edge_temporal((int(m[0]), int(m[1]), int(m[2]), int(t)), hop = hop, max_span = span_t, and_or = and_or, max_step = step, aux_score = aux_score, dsa = distribution_aware, max_rule = max_rule, file_type='test')\n",
    "    pred = sigmoid(raw_score)\n",
    "\n",
    "    return [pred, label, m, project_rules]\n",
    "\n",
    "def missing_detect_helper(fact_label_t):\n",
    "    m = fact_label_t[0]\n",
    "    label = fact_label_t[1]\n",
    "    t = fact_label_t[2]\n",
    "    raw_score = detector.score_edge_missing((int(m[0]), int(m[1]), int(m[2]), int(t)), hop_c = hop, max_span_c = span, max_span_t = span_m, and_or = and_or, max_step = step, aux_score = aux_score_m, dsa = distribution_aware)\n",
    "    pred = sigmoid(-raw_score)\n",
    "\n",
    "    return [pred, label]\n",
    "\n",
    "p = mp.Pool(50)\n",
    "# update model in validate set\n",
    "for t in tqdm(valid_t_2_pos.keys()):\n",
    "    pos_samples = valid_t_2_pos[t]\n",
    "    #p.map(update_helper_valid, pos_samples)\n",
    "\n",
    "# detect anomalies in test set\n",
    "for t in tqdm(list(test_t_2_C_neg.keys())[:10]):\n",
    "    pos_samples = test_t_2_pos[t]\n",
    "    pos_missing = test_t_2_M_neg[1][t]\n",
    "\n",
    "    tmp_pred_C = []\n",
    "    tmp_label_C = []\n",
    "    tmp_pred_T = []\n",
    "    tmp_label_T = []\n",
    "    tmp_pred_M = []\n",
    "    tmp_label_M = []\n",
    "\n",
    "    concept_temporal_p = [[pos_samples[i], 0, t] for i in range(len(pos_samples))]\n",
    "    missing_p = [[pos_missing[i], 1, t] for i in range(len(pos_missing))]\n",
    "    concept_score_label_p = p.map(concept_detect_helper, concept_temporal_p)\n",
    "    temporal_score_label_proj_p = p.map(temporal_detect_helper, concept_temporal_p)\n",
    "    missing_score_label_p = p.map(missing_detect_helper, missing_p)\n",
    "    for i in range(len(concept_score_label_p)):\n",
    "        pred_list_C.append(concept_score_label_p[i][0])\n",
    "        label_list_C.append(concept_score_label_p[i][1])\n",
    "        tmp_pred_C.append(concept_score_label_p[i][0])\n",
    "        tmp_label_C.append(concept_score_label_p[i][1])\n",
    "    \n",
    "    for i in range(len(temporal_score_label_proj_p)):\n",
    "        pred_list_T.append(temporal_score_label_proj_p[i][0])\n",
    "        label_list_T.append(temporal_score_label_proj_p[i][1])\n",
    "        all_proj_rules.append([temporal_score_label_proj_p[i][2], temporal_score_label_proj_p[i][3]])\n",
    "        tmp_pred_T.append(temporal_score_label_proj_p[i][0])\n",
    "        tmp_label_T.append(temporal_score_label_proj_p[i][1])\n",
    "    \n",
    "    for i in range(len(missing_score_label_p)):\n",
    "        pred_list_M.append(missing_score_label_p[i][0])\n",
    "        label_list_M.append(missing_score_label_p[i][1])\n",
    "        tmp_pred_M.append(missing_score_label_p[i][0])\n",
    "        tmp_label_M.append(missing_score_label_p[i][1])\n",
    "    \n",
    "\n",
    "    concept_n = [[test_t_2_C_neg[t][i], 1, t] for i in range(len(test_t_2_C_neg[t]))]\n",
    "    temporal_n = [[test_t_2_T_neg[t][i], 1, t] for i in range(len(test_t_2_T_neg[t]))]\n",
    "    missing_n = [[test_t_2_M_neg[0][t][i], 0, t] for i in range(len(test_t_2_M_neg[0][t]))]\n",
    "    concept_score_label_n = p.map(concept_detect_helper, concept_n)\n",
    "    temporal_score_label_proj_n = p.map(temporal_detect_helper, temporal_n)\n",
    "    missing_score_label_n = p.map(missing_detect_helper, missing_n)\n",
    "    \n",
    "\n",
    "    for i in range(len(concept_score_label_n)):\n",
    "        pred_list_C.append(concept_score_label_n[i][0])\n",
    "        label_list_C.append(concept_score_label_n[i][1])\n",
    "        tmp_pred_C.append(concept_score_label_n[i][0])\n",
    "        tmp_label_C.append(concept_score_label_n[i][1])\n",
    "    \n",
    "    for i in range(len(temporal_score_label_proj_n)):\n",
    "        pred_list_T.append(temporal_score_label_proj_n[i][0])\n",
    "        label_list_T.append(temporal_score_label_proj_n[i][1])\n",
    "        tmp_pred_T.append(temporal_score_label_proj_n[i][0])\n",
    "        tmp_label_T.append(temporal_score_label_proj_n[i][1])\n",
    "    \n",
    "    for i in range(len(missing_score_label_n)):\n",
    "        pred_list_M.append(missing_score_label_n[i][0])\n",
    "        label_list_M.append(missing_score_label_n[i][1])\n",
    "        tmp_pred_M.append(missing_score_label_n[i][0])\n",
    "        tmp_label_M.append(missing_score_label_n[i][1])\n",
    "    \n",
    "    time_specific_pred_list_C.append(tmp_pred_C)\n",
    "    time_specific_label_list_C.append(tmp_label_C)\n",
    "    time_specific_pred_list_T.append(tmp_pred_T)\n",
    "    time_specific_label_list_T.append(tmp_label_T)\n",
    "    time_specific_pred_list_M.append(tmp_pred_M)\n",
    "    time_specific_label_list_M.append(tmp_label_M)\n",
    "    \n",
    "    # update model in test set\n",
    "    p.map(update_helper_test, all_proj_rules)\n",
    "    all_proj_rules = []\n",
    "\n",
    "get_metric(np.array(pred_list_C), np.array(label_list_C), 'concept') # concept\n",
    "get_metric(np.array(pred_list_T), np.array(label_list_T), 'time') # time\n",
    "get_metric(np.array(pred_list_M), np.array(label_list_M), 'missing') # missing\n",
    "detector.re_fresh()\n",
    "p.close()\n",
    "# 19min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
